{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "colorization",
   "display_name": "Colorization"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import get_default_graph\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from tensorflow.keras.layers import concatenate, Conv2D, Input, UpSampling2D, RepeatVector, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def process_path(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    grayscale_img = decode_grayscale_img(img)\n",
    "    #inception_embeded = create_inception_embedding(grayscale_img)\n",
    "    yuv_img = decode_yuv_img(img)\n",
    "    return grayscale_img, yuv_img\n",
    "    #return ([grayscale_img, inception_embeded], yuv_img)\n",
    "\n",
    "\n",
    "def decode_grayscale_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # convert to grayscale\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "\n",
    "def decode_yuv_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # convert to yuv colorspace\n",
    "    img = tf.image.rgb_to_yiq(img)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "\n",
    "def create_inception_embedding(grayscaled_img):\n",
    "    grayscaled_img = tf.image.resize(grayscaled_img, [299, 299])\n",
    "    grayscaled_img = tf.image.grayscale_to_rgb(grayscaled_img)\n",
    "    grayscaled_img = tf.expand_dims(grayscaled_img, 0)\n",
    "    grayscaled_img = preprocess_input(grayscaled_img)\n",
    "    \n",
    "    print(grayscaled_img.shape)\n",
    "    # grayscaled_rgb_resized = []\n",
    "    # for i in grayscaled_rgb:\n",
    "    #     i = resize(i, (299, 299, 3), mode='constant')\n",
    "    #     grayscaled_rgb_resized.append(i)\n",
    "    # grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    # grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    \n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_img, steps=1)\n",
    "    return embed\n",
    "\n",
    "\n",
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "    \n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def show_batch(image_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    print(image_batch[0])\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        #plt.imshow(image_batch[n][:,:,0], cmap='binary')\n",
    "        plt.imshow(image_batch[n])\n",
    "        print(image_batch[n].shape)\n",
    "        plt.axis('off')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and/or load weights for InceptionResNetV2\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image paths\n",
    "raw_ds = tf.data.Dataset.list_files('./Train/*')\n",
    "\n",
    "# Process paths and decode images (RGB)\n",
    "labeled_ds = raw_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = prepare_for_training(labeled_ds)\n",
    "\n",
    "#gray_batch, lab_batch = next(iter(train_ds))\n",
    "#show_batch(lab_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up neural network\n",
    "\n",
    "# Encoder part\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "# Fusion part\n",
    "#embed_input = Input(shape=(1000,))\n",
    "# fusion_output = RepeatVector(32 * 32)(embed_input)\n",
    "# fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "# fusion_output = concatenate([encoder_output, fusion_output], axis=3)\n",
    "# fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n",
    "\n",
    "# Decoder part\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "#model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n",
    "model = Model(inputs=encoder_input, outputs=decoder_output)\n",
    "model.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7a875f4d503b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\d5z6\\documents\\colorization\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\d5z6\\documents\\colorization\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m           \u001b[0msteps_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'steps_per_epoch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m           epochs=0)\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m       steps_per_epoch = (\n",
      "\u001b[1;32mc:\\users\\d5z6\\documents\\colorization\\venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36minfer_steps_for_dataset\u001b[1;34m(model, dataset, steps, epochs, steps_name)\u001b[0m\n\u001b[0;32m   1750\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcardinality\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFINITE\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m     raise ValueError('When passing an infinitely repeating dataset, you '\n\u001b[1;32m-> 1752\u001b[1;33m                      'must specify the `%s` argument.' % (steps_name,))\n\u001b[0m\u001b[0;32m   1753\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument."
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}